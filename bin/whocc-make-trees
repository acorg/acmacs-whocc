#! /usr/bin/env python3
import sys, os, re, datetime, subprocess, shutil, concurrent.futures, time, random, logging
from pathlib import Path
from acmacs_py import email

class Error (RuntimeError): pass

# ======================================================================

def main(args):
    check_cwd()
    working_dir = Path(args.subtype)
    working_dir.mkdir(exist_ok=True)
    working_dir = working_dir.resolve()
    copy_seqdb()
    source_fas = export_sequences(subtype=args.subtype, working_dir=working_dir)
    # fork
    with email.send_after(subject=f"{Path(sys.argv[0]).name} {args.subtype} {working_dir.name}") as email_data:
        best_tree_file = raxml_ng(subtype=args.subtype, working_dir=working_dir, source_fas=source_fas)
        best_tree_file = garli(subtype=args.subtype, working_dir=working_dir, best_tree_file=best_tree_file, source_fas=source_fas)
        tjz, pdf = generate_results(subtype=args.subtype, working_dir=working_dir, best_tree_file=best_tree_file)
        email_data["body"] += f"{tjz}\n\n{pdf}\n\n{str(pdf).replace('/syn/eu/ac/results/', 'https://notebooks.antigenic-cartography.org/eu/results/')}"

# ======================================================================

CONFIG = {
    "h1": {
        "export": {
            "base": "AH1N1/SOUTH_AUSTRALIA/30/2013_MDCK1_h67695CD8",  # 2009-04-09
            "prepend": [
                "AH1N1/MICHIGAN/45/2015_QMC2MDCK3",                   # 2015-09-07
            ],
            "exclude": [
                # 2020-08-10 Sarah: They're not sequenced by a CC or NIC, but by a company.  The sequences don't have obvious errors, but are very different (obviously) from the rest. My feeling is they're either variant viruses (from swine) or a sequencing error.
                "AH1N1/ANKARA/14015-724/2019_hC7E3C2CC",
                "AH1N1/ANKARA/14017-004/2019_h55177F82",
                "AH1N1/ANKARA/14015-736/2019_hF435606A",
                # 2021-06-22 deletion at the end
                "AH1N1/BADEN-WURTTEMBERG/1/2020_OR_h287EF0FC",
            ],
            "args": ["--flu", "h1", "--recent-matched", "5000,3000", "--remove-nuc-duplicates", "--host", "human", "--most-common-length", "--nuc-hamming-distance-mean-threshold", "140"],
        },
        "raxml-ng": {
            "slurm-nodelist": "i20",
        },
        "garli": {
            "slurm-nodelist": "i20",
            "required_memory": 3000,
        },
    },

    "h3": {
        "export": {
            "base": "AH3N2/PERTH/16/2009_E3/E2_h6BB67ED5",        # 2009-04-07    3C.3
            "prepend": [
                "AH3N2/VICTORIA/361/2011_MDCK2_h160B8E64",        # 2011-10-24    3C.3
                # [2020-09-07] Nearest common ancestor of 2a and 3a in /syn/eu/ac/results/eu/2019-0417-h3-trees/cdc-usa-2009-2019-250-per-year-1.rough-29532.tree.pdf https://notebooks.antigenic-cartography.org/eu/results/eu/2019-0417-h3-trees/
                "AH3N2/MARYLAND/30/2012_Rx/MDCK2_hCD1FF690",          # 2012-07-26    3C.3
                # [2020-08-17] Derek thinks that the root (A/STOCKHOLM/6/2014) is not old enough
                "AH3N2/TEXAS/50/2012_E6_h7B522BBD",                   # 2012-04-15    3C.3  root in MELB tree 2020
                "AH3N2/VERMONT/6/2012_MK1/MDCK2_h6BD4A807",
                "AH3N2/STOCKHOLM/6/2014_OR",                          # 2014-02-06    3a
                # [2020-02-07] intermediate strains from nextflu https://nextstrain.org/flu/seasonal/h3n2/ha/2y to keep 3a at the top
                "AH3N2/SWITZERLAND/9715293/2013_SIAT1/SIAT2/SIAT1",   # 2013-12-06    3a
                "AH3N2/NORWAY/466/2014_SIAT1SIAT1/SIAT1",             # 2014-02-03    3a
                "AH3N2/SOUTH_AUSTRALIA/55/2014_MDCK1/",               # 2014-06-29    3a
                "AH3N2/TASMANIA/11/2014_MDCK1",                       # 2014-03-16    3a
                "AH3N2/KOBE/63/2014_MDCK1",                           # 2014-05-21    3a
                "AH3N2/PERU/27/2015_MDCK2/SIAT1",                     # 2015-04-13    3a
                "AH3N2/NEVADA/22/2016_OR",                            # 2016-03-05    3a
                "AH3N2/IDAHO/33/2016_OR",                             # 2016-06-08    3a
                "AH3N2/TEXAS/88/2016_OR",                             # 2016-02-25    3a
                "AH3N2/TEXAS/71/2017_OR",                             # 2017-03-18    3a
                "AH3N2/BRAZIL/7331/2018_OR",                          # 2018-07-09    3a
                "AH3N2/KANSAS/14/2017_OR_h",                          # 2017-12-14    3a, to have serum circles in the sig page
                "AH3N2/HONG_KONG/4801/2014_MDCK4/SIAT4",              # 2014-02-26    2a
                "AH3N2/HAWAII/47/2014_SIAT3",                         # 2014-07-18    2a
                "AH3N2/NORTH_CAROLINA/4/2017_OR",                     # 2017-01-26    2a2
                "AH3N2/NORTH_CAROLINA/4/2016_OR",                     # 2016-01-14    2a1
                "AH3N2/ANTANANARIVO/1067/2016_OR",                    # 2016-04-06    2a1
                "AH3N2/HONG_KONG/2286/2017_MDCKx/SIAT1",              # 2017-05-23    2a1b 135K
                "AH3N2/WISCONSIN/327/2017_OR",                        # 2017-09-22    2a1b 135K
                "AH3N2/ABU_DHABI/240/2018_OR",                        # 2018-01-01    2a1b 135K
                "AH3N2/JAMAICA/1447/2018_OR",                         # 2018-02-19    2a1b 131K

                # Strains before and after T135N substitution to have a better 135N branch placement
                # Sarah 2021-02-08 17:05
                "AH3N2/WISCONSIN/85/2016_OR_h2A1A947B",
                "AH3N2/SRI_LANKA/56/2017_SIAT1_hBE9667B3",
                "AH3N2/SOUTH_CAROLINA/4/2017_OR_hC91E2A2E",
                "AH3N2/YOKOHAMA/145/2017_A2/SIAT1_h0318BAD4",
                "AH3N2/INDIA/9930/2017_X2_h92EF43A1",
                "AH3N2/HONG_KONG/3118/2017_OR_h84D43D73",
                "AH3N2/HAWAII/47/2014_QMC2SIAT1_h3E8BB38B",
                "AH3N2/NIIGATA-C/43/2015_MDCK2/SIAT1_h142B6AE9",
                "AH3N2/DAKAR/17/2016_X1/SIAT1_h19A41C86",
                # "AH3N2/CAMEROON/16V-9267/2016_OR_hD54E7459", # excluded, truncated sequence
                "AH3N2/LAOS/3008/2016_MDCK2_h5F21B56F",
                "AH3N2/YUNNAN_LINXIANG/1718/2016_MDCK1/SIAT1_hF485FA1E",
                "AH3N2/HONG_KONG/2302/2016_OR_h0A3FDAB6",
                "AH3N2/ONTARIO/RV2414/2015_X1_h7F1CB896",
                "AH3N2/ONTARIO/RV2414/2015_X1/SIAT2_hE8F24BAB",
                "AH3N2/HONG_KONG/2286/2017_E8_hA2DFD717",
                "AH3N2/HONG_KONG/2286/2017_E7/E2_h981D5A8D",
                "AH3N2/HONG_KONG/2286/2017_E7_h6B9A08A4",
                "AH3N2/HONG_KONG/2286/2017_EGG_CLN_23-74_hCF491C22",
                "AH3N2/HONG_KONG/2286/2017_E7/E1_h2E64DCBC",
            ],
            # before 2020-02-10  "args": ["--flu", "h3", "--recent-matched", "3000,4000", "--host", "human"],
            "args": ["--flu", "h3", "--recent-matched", "5000,3000", "--remove-nuc-duplicates", "--host", "human", "--length", "1650", "--minimum-nuc-length", "1650", "--nuc-hamming-distance-mean-threshold", "140"],
        },
        "raxml-ng": {
            "slurm-nodelist": "i22",
        },
        "garli": {
            "slurm-nodelist": "i22",
            "required_memory": 7000,
        },
    },

    "bvic": {
        "export": {
            "base": "B/VICTORIA/830/2013_MDCK2_hA7D58234", # MELB: B/BRISBANE/60/2008, B/CHONGQING BANAN/1840/2017 (V1A)
            "prepend": [],
            "exclude": [
                # 6DEL2019
                "B/YOKOHAMA/1/2019_h27650573",
                "B/KANAGAWA/ZC1825/2019_hF6E9C34C",
                "B/NAGASAKI/579/2019_h54DB0674",
                "B/SAITAMA/1/2019_h1E975EEA",
                "B/KANAGAWA/AC1867/2019_hB8B69E0F",
                # 4-del or garbage in the middle
                "B/MIE/1/2019_h2C1CA3AE",
                ],
            "args": ["--flu", "b", "--lineage", "vic", "--recent", "8000", "--remove-nuc-duplicates", "--length", "1710", "--minimum-nuc-length", "1710", "--nuc-hamming-distance-mean-threshold", "140"],
        },
        "raxml-ng": {
            "slurm-nodelist": "i21",
        },
        "garli": {
            "slurm-nodelist": "i21",
            "required_memory": 3000,
        },
    },

    "byam": {
        "export": {
            "base": "B/MINNESOTA/2/2014_MDCK1_hF5650512", # MELB uses B/Florida/4/2006 as base (Y1 clade), B/MASSACHUSETTS/02/2012 egg (Y2), B/HUBEI WUJIAGANG/20158/2009
            "prepend": ["B/IDAHO/1/2014_MDCK1_h45F759E9", "B/MASSACHUSETTS/7/2014_MK1/MDCK1_h42B3D3E5"],
            "args": ["--flu", "b", "--lineage", "yam", "--recent", "8000", "--remove-nuc-duplicates", "--length", "1710", "--minimum-nuc-length", "1710", "--nuc-hamming-distance-mean-threshold", "140"],
        },
        "raxml-ng": {
            "slurm-nodelist": "i18",
        },
        "garli": {
            "slurm-nodelist": "i18",
            "required_memory": 3000,
        },
    },

    # ----------------------------------------------------------------------

    "raxml-ng": {
        "num_runs": 16,
    },

    "garli": {
        "num_runs": 16,

        # the number of attachment branches evaluated for each taxon to be added to the tree during the creation of an ML stepwise-addition starting tree (when garli is run without raxml step)
        "attachmentspertaxon": 1000000,

        # termination condition: the maximum number of seconds for the run to continue (default value is a week)
        "stoptime": 60*60,

        # termination condition: when no new significantly better scoring topology has been encountered in greater than this number of generations, garli stops
        "genthreshfortopoterm": 20000,
    },

}

SEQDB3_COMMON_ARGS = ["--db", "../seqdb.json.xz", "--name-format", "{seq_id}", "--nucs", "--wrap", "--remove-with-front-back-deletions"]

# ======================================================================

def copy_seqdb():
    source_seqdb = Path("/syn/eu/AD/data/seqdb.json.xz")
    target_seqdb = Path("seqdb.json.xz")
    if not target_seqdb.exists():
        logging.info(f"copying seqdb")
        shutil.copy(source_seqdb, target_seqdb)
    subprocess.check_call(["ls", "-l", str(target_seqdb)])

# ----------------------------------------------------------------------

def export_sequences(subtype :str, working_dir :Path):
    source_fas = working_dir.joinpath("source.fas")
    if not source_fas.exists():
        data = CONFIG[subtype]["export"]
        if data.get("exclude"):
            exclude = " ".join(f"--exclude-seq-id '{name}'" for name in data["exclude"])
        else:
            exclude = ""
        if data.get("prepend"):
            prepend = " ".join(f"--prepend '{name}'" for name in data["prepend"])
        else:
            prepend = ""
        command = f"""seqdb3 '{"' '".join(SEQDB3_COMMON_ARGS)}' '{"' '".join(data["args"])}' --base-seq-id '{data["base"]}' {exclude} {prepend} --fasta source.fas"""

        script_path = working_dir.joinpath("export.sh")
        with script_path.open("w") as script:
            script.write(f"#! /bin/bash\n{command}\n")
        script_path.chmod(0o755)

        logging.info(f"running export.sh in {working_dir}")
        subprocess.check_call([str(script_path.resolve())], cwd=working_dir)
    subprocess.check_call(["ls", "-l", str(source_fas)])
    return source_fas

# ----------------------------------------------------------------------

sTreeRootDir = Path("/syn/eu/ac/results/trees")

def check_cwd():
    if Path(os.getcwd()).parent != sTreeRootDir:
        raise Error(f"invalid CWD, subdir of {sTreeRootDir} expected, e.g. {sTreeRootDir.joinpath(datetime.date.today().strftime('%Y-%m%d'))}")

# ======================================================================

sRandomGen = random.SystemRandom()

def raxml_ng(subtype, working_dir, source_fas):

    output_dir = get_output_dir(working_dir, "raxml-ng.")

    raxml_cmd = [
        "/syn/bin/raxml-ng",
        "--search",         # (raxml: -f d) run topology search to find the best-scoring ML tree
        "--model", "GTR+G+I", # raxml: -m GTRGAMMAI -c 4
        "--msa", str(source_fas),
        "--msa-format", "FASTA",
        "--outgroup", CONFIG[subtype]["export"]["base"],
        "--tree", "pars{1}",
        "--log", "PROGRESS", # VERBOSE, DEBUG
        "--threads", "1",
    ]
    # "--silent", "--no-seq-check"
    srun_cmd = [
        "srun",
        "--cpus-per-task=2",
        "--nodes=1",
        "--ntasks=1",
        "--threads=1",
        f"--nodelist={CONFIG[subtype]['raxml-ng']['slurm-nodelist']}",
    ]

    # ----------------------------------------------------------------------

    def prefix(run_id):
        return f"{output_dir.joinpath(run_id)}"

    def run_raxml_ng(run_id):
        cmd = (srun_cmd + [f"--job-name=raxml-ng {subtype} {run_id}", f"--output={output_dir.joinpath(run_id).with_suffix('.stdout')}", f"--error={output_dir.joinpath(run_id).with_suffix('.stderr')}"]
               + raxml_cmd + ["--prefix", prefix(run_id), "--seed", str(sRandomGen.randint(1, 0xFFFFFFF))])
        with output_dir.joinpath("commands.txt").open("a") as commands_txt:
            print(" ".join(cmd), file=commands_txt)
        # print(cmd)
        subprocess.check_call(cmd)

    # ----------------------------------------------------------------------

    def submit():
        failures = 0
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future_to_runid = {executor.submit(run_raxml_ng, run_id): run_id for run_id in (f"{ri:03d}" for ri in range(CONFIG["raxml-ng"]["num_runs"]))}
            for future in concurrent.futures.as_completed(future_to_runid):
                try:
                    future.result()
                except Exception as err:
                    logging.error(f"raxml-ng {subtype} {future_to_runid[future]} FAILED: {err}")
                    failures += 1
                else:
                    logging.info(f"raxml-ng run {subtype} {future_to_runid[future]} completed")
        if failures:
            logging.error(f"raxml-ng {subtype} FAILED: {failures} runs failed")
        else:
            logging.info("raxml-ng completed")

    # ----------------------------------------------------------------------

    def results_available():
        return len(list(output_dir.glob("*.raxml.bestTree"))) > (CONFIG["raxml-ng"]["num_runs"] / 2)

    # ----------------------------------------------------------------------

    sBestScore = re.compile(r"Final LogLikelihood: -([\d\.]+)", re.I)

    def find_best_result():
        # scores in results are positive numbers
        results = [[log_file_name, float(sBestScore.search(log_file_name.open().read()).group(1))] for log_file_name in output_dir.glob("*.raxml.log")]
        best = min(results, key=lambda en: en[1])
        return [best[0].with_suffix(".bestTree"), best[1]]

    # ----------------------------------------------------------------------

    if not results_available():
        start = datetime.datetime.now()
        submit()
        log(working_dir, f"raxml-ng elapsed: {datetime.datetime.now() - start}")
    best_tree_file, best_score = find_best_result()
    log(working_dir, f"raxml-ng best: {best_tree_file}  score: {best_score}")
    return best_tree_file

# ======================================================================

def garli(subtype, working_dir, best_tree_file, source_fas):

    output_dir = get_output_dir(working_dir, "garli.")

    garli_cmd = "/syn/bin/garli"
    srun_cmd = [
        "srun",
        "--cpus-per-task=2",
        "--nodes=1",
        "--ntasks=1",
        "--threads=1",
        f"--nodelist={CONFIG[subtype]['garli']['slurm-nodelist']}",
    ]

    # ----------------------------------------------------------------------

    def make_conf(run_id, required_memory):
        """Returns filename of the written conf file"""
        # if not outgroup or not isinstance(outgroup, list) or not all(isinstance(e, int) and e > 0 for e in outgroup):
        #     raise ValueError("outgroup must be non-empty list of taxa indices in the fasta file starting with 1")
        global GARLI_CONF, CONFIG
        garli_args = {
            "source": str(source_fas.resolve()),
            "availablememory": required_memory or CONFIG[subtype]["garli"]["required_memory"],
            "streefname": str(best_tree_file.resolve()),
            "output_prefix": str(output_dir.joinpath(run_id)),
            "attachmentspertaxon": CONFIG["garli"]["attachmentspertaxon"],
            "randseed": sRandomGen.randint(1, 0xFFFFFFF),
            "genthreshfortopoterm": CONFIG["garli"]["genthreshfortopoterm"],
            "searchreps": 1,
            "stoptime": CONFIG["garli"]["stoptime"],
            "outgroup": "1" # " ".join(str(e) for e in outgroup),
            }
        conf = GARLI_CONF.format(**garli_args)
        conf_filename = output_dir.joinpath(run_id + ".garli.conf")
        with conf_filename.open("w") as f:
            f.write(conf)
        return conf_filename

    # ----------------------------------------------------------------------

    def find_memory_requirements():
        sGreat = re.compile(r"\s*great\s+>=\s+(\d+)\s+MB", re.I)
        conf_file = make_conf(run_id="find_memory_requirements", required_memory=None)
        start = time.time()
        proc = subprocess.Popen([garli_cmd, str(conf_file)], stdin=subprocess.DEVNULL, stdout=subprocess.PIPE, stderr=subprocess.STDOUT)
        required_memory = None # default, in case the code below fails or times out
        timeout = 60
        while required_memory is None:
            line = proc.stdout.readline().decode("utf-8")
            if mm := sGreat.match(line):
                required_memory = int(mm.group(1))
            elif (time.time() - start) > timeout:
                required_memory = int(CONFIG[subtype]["garli"]["required_memory"])
                module_logger.warning(f"Cannot obtain memory required by garli in {timeout} seconds: default for {subtype}: {required_memory} will be used")
        proc.kill()
        return required_memory

    # ----------------------------------------------------------------------

    def run_garli(run_id, required_memory):
        conf_file = make_conf(run_id=run_id, required_memory=required_memory)
        # --mem-per-cpu is half of required_memory because we allocate whole 2 cpus per run to prevent from hyperthreading
        cmd = (srun_cmd + [f"--mem-per-cpu={int(required_memory / 2) + 1}", f"--job-name='garli {subtype} {run_id}'", f"--output={output_dir.joinpath(run_id).with_suffix('.stdout')}", f"--error={output_dir.joinpath(run_id).with_suffix('.stderr')}"]
               + [garli_cmd, str(conf_file)])
        with output_dir.joinpath("commands.txt").open("a") as commands_txt:
            print(" ".join(cmd), file=commands_txt)
        # print(cmd)
        subprocess.check_call(cmd)

    # ----------------------------------------------------------------------

    def submit():
        required_memory = find_memory_requirements()
        failures = 0
        with concurrent.futures.ThreadPoolExecutor() as executor:
            future_to_runid = {executor.submit(run_garli, run_id, required_memory): run_id for run_id in (f"{ri:03d}" for ri in range(CONFIG["garli"]["num_runs"]))}
            for future in concurrent.futures.as_completed(future_to_runid):
                try:
                    future.result()
                except Exception as err:
                    logging.error(f"garli {subtype} {future_to_runid[future]} FAILED: {err}")
                    failures += 1
                else:
                    logging.info(f"garli run {subtype} {future_to_runid[future]} completed")
        if failures:
            logging.error(f"garli {subtype} FAILED: {failures} runs failed")
        else:
            logging.info("garli completed")

    # ----------------------------------------------------------------------

    def results_available():
        return len(list(output_dir.glob("*.best.phy"))) > (CONFIG["garli"]["num_runs"] / 2)

    # ----------------------------------------------------------------------

    sReFinalScore = re.compile(r"Final\t-(\d+\.\d+)")

    def find_best_result():
        # scores in results are positive numbers
        results = [[log_file_name, float(sReFinalScore.search(log_file_name.open().read()).group(1))] for log_file_name in output_dir.glob("*.log00.log")]
        if not results:
            raise Error(f"no results found in {output_dir}/*.log00.log")
        best = min(results, key=lambda en: en[1])
        return [Path(str(best[0]).replace(".log00.log", ".best.phy")), best[1]]

    # ----------------------------------------------------------------------

    if not results_available():
        start = datetime.datetime.now()
        submit()
        log(working_dir, f"garli elapsed: {datetime.datetime.now() - start}")
    best_tree_file, best_score = find_best_result()
    log(working_dir, f"garli best: {best_tree_file}  score: {best_score}")
    return best_tree_file

# ----------------------------------------------------------------------

GARLI_CONF = """\
[general]
datafname = {source}
streefname = {streefname}
# Prefix of all output filenames, such as log, treelog, etc. Change
# this for each run that you do or the program will overwrite previous
# results.
ofprefix = {output_prefix}

constraintfile = none

# (1 to infinity) The number of attachment branches evaluated for each
# taxon to be added to the tree during the creation of an ML
# stepwise-addition starting tree. Briefly, stepwise addition is an
# algorithm used to make a tree, and involves adding taxa in a random
# order to a growing tree. For each taxon to be added, a number of
# randomly chosen attachment branches are tried and scored, and then
# the best scoring one is chosen as the location of that taxon. The
# attachmentspertaxon setting controls how many attachment points are
# evaluated for each taxon to be added. A value of one is equivalent
# to a completely random tree (only one randomly chosen location is
# evaluated). A value of greater than 2 times the number of taxa in
# the dataset means that all attachment points will be evaluated for
# each taxon, and will result in very good starting trees (but may
# take a while on large datasets). Even fairly small values (< 10) can
# result in starting trees that are much, much better than random, but
# still fairly different from one another. Default: 50.
attachmentspertaxon = {attachmentspertaxon}

# -1 - random seed chosen automatically
randseed = {randseed}

# in Mb
availablememory = {availablememory}

# The frequency with which the best score is written to the log file, default: 10
logevery = 1000

# Whether to write three files to disk containing all information
# about the current state of the population every saveevery
# generations, with each successive checkpoint overwriting the
# previous one. These files can be used to restart a run at the last
# written checkpoint by setting the restart configuration entry. Default: 0
writecheckpoints = 0

# Whether to restart at a previously saved checkpoint. To use this
# option the writecheckpoints option must have been used during a
# previous run. The program will look for checkpoint files that are
# named based on the ofprefix of the previous run. If you intend to
# restart a run, NOTHING should be changed in the config file except
# setting restart to 1. A run that is restarted from checkpoint will
# give exactly the same results it would have if the run had gone to
# completion. Default: 0
restart = 0

# If writecheckpoints or outputcurrentbesttopology are specified, this
# is the frequency (in generations) at which checkpoints or the
# current best tree are written to file. default: 100
saveevery = 1000

# Specifies whether some initial rough optimization is performed on
# the starting branch lengths and alpha parameter. This is always
# recommended. Default: 1.
refinestart = 1

# If true, the current best tree of the current search replicate is
# written to <ofprefix>.best.current.tre every saveevery
# generations. In versions before 0.96 the current best topology was
# always written to file, but that is no longer the case. Seeing the
# current best tree has no real use apart from satisfying your
# curiosity about how a run is going. Default: 0.
outputcurrentbesttopology = 0

# If true, each new topology encountered with a better score than the
# previous best is written to file. In some cases this can result in
# really big files (hundreds of MB) though, especially for random
# starting topologies on large datasets. Note that this file is
# interesting to get an idea of how the topology changed as the
# searches progressed, but the collection of trees should NOT be
# interpreted in any meaningful way. This option is not available
# while bootstrapping. Default: 0.
outputeachbettertopology = 0

# Specifies whether the automatic termination conditions will be
# used. The conditions specified by both of the following two
# parameters must be met. See the following two parameters for their
# definitions. If this is false, the run will continue until it
# reaches the time (stoptime) or generation (stopgen) limit. It is
# highly recommended that this option be used! Default: 1.
enforcetermconditions = 1

# This specifies the first part of the termination condition. When no
# new significantly better scoring topology (see significanttopochange
# below) has been encountered in greater than this number of
# generations, this condition is met. Increasing this parameter may
# improve the lnL scores obtained (especially on large datasets), but
# will also increase runtimes. Default: 20000.
genthreshfortopoterm = {genthreshfortopoterm}

# The second part of the termination condition. When the total
# improvement in score over the last intervallength x intervalstostore
# generations (default is 500 generations, see below) is less than
# this value, this condition is met. This does not usually need to be
# changed. Default: 0.05
scorethreshforterm = 0.05

# The lnL increase required for a new topology to be considered
# significant as far as the termination condition is concerned. It
# probably doesn’t need to be played with, but you might try
# increasing it slightly if your runs reach a stable score and then
# take a very long time to terminate due to very minor changes in
# topology. Default: 0.01
significanttopochange = 0.01

# Whether a phylip formatted tree files will be output in addition to
# the default nexus files for the best tree across all replicates
# (<ofprefix>.best.phy), the best tree for each replicate
# (<ofprefix>.best.all.phy) or in the case of bootstrapping, the best
# tree for each bootstrap replicate (<ofprefix.boot.phy>).
# We use .phy (it's newick tree format), use 1 here!
outputphyliptree = 1

# Whether to output three files of little general interest: the
# “fate”, “problog” and “swaplog” files. The fate file shows the
# parentage, mutation types and scores of every individual in the
# population during the entire search. The problog shows how the
# proportions of the different mutation types changed over the course
# of the run. The swaplog shows the number of unique swaps and the
# number of total swaps on the current best tree over the course of
# the run. Default: 0
outputmostlyuselessfiles = 0

# This option allow for orienting the tree topologies in a consistent
# way when they are written to file. Note that this has NO effect
# whatsoever on the actual inference and the specified outgroup is NOT
# constrained to be present in the inferred trees. If multiple
# outgroup taxa are specified and they do not form a monophyletic
# group, this setting will be ignored. If you specify a single
# outgroup taxon it will always be present, and the tree will always
# be consistently oriented. To specify an outgroup consisting of taxa
# 1, 3 and 5 the format is this: outgroup = 1 3 5
outgroup = {outgroup}

resampleproportion = 1.0
inferinternalstateprobs = 0
outputsitelikelihoods = 0
optimizeinputonly = 0
collapsebranches = 1

# The number of independent search replicates to perform during a
# program execution. You should always either do multiple search
# replicates or multiple program executions with any dataset to get a
# feel for whether you are getting consistent results, which suggests
# that the program is doing a decent job of searching. Note that if
# this is > 1 and you are performing a bootstrap analysis, this is the
# number of search replicates to be done per bootstrap replicate. That
# can increase the chance of finding the best tree per bootstrap
# replicate, but will also increase bootstrap runtimes
# enormously. Default: 2
searchreps = {searchreps}

bootstrapreps = 0

# ------------------- FOR NUCLEOTIDES --------------
datatype=nucleotide

# The number of relative substitution rate parameters (note that the
# number of free parameters is this value minus one). Equivalent to
# the “nst” setting in PAUP* and MrBayes. 1rate assumes that
# substitutions between all pairs of nucleotides occur at the same
# rate, 2rate allows different rates for transitions and
# transversions, and 6rate allows a different rate between each
# nucleotide pair. These rates are estimated unless the fixed option
# is chosen. New in version 0.96, parameters for any submodel of the
# GTR model may now be estimated. The format for specifying this is
# very similar to that used in the “rclass’ setting of PAUP*. Within
# parentheses, six letters are specified, with spaces between
# them. The six letters represent the rates of substitution between
# the six pairs of nucleotides, with the order being A-C, A-G, A-T,
# C-G, C-T and G-T. Letters within the parentheses that are the same
# mean that a single parameter is shared by multiple nucleotide
# pairs. For example, ratematrix = (a b a a b a) would specify the HKY
# 2-rate model (equivalent to ratematrix = 2rate). This entry,
# ratematrix = (a b c c b a) would specify 3 estimated rates of
# subtitution, with one rate shared by A-C and G-T substitutions,
# another rate shared by A-G and C-T substitutions, and the final rate
# shared by A-T and C-G substitutions. Default: 6rate
ratematrix = 6rate

# (equal, empirical, estimate, fixed) Specifies how the equilibrium
# state frequencies (A, C, G and T) are treated. The empirical setting
# fixes the frequencies at their observed proportions, and the other
# options should be self-explanatory. Default: estimate
statefrequencies = estimate

# (none, gamma, gammafixed) – The model of rate heterogeneity
# assumed. “gammafixed” requires that the alpha shape parameter is
# provided, and a setting of “gamma” estimates it. Default: gamma
ratehetmodel = gamma

# (1 to 20) – The number of categories of variable rates (not
# including the invariant site class if it is being used). Must be set
# to 1 if ratehetmodel is set to none. Note that runtimes and memory
# usage scale linearly with this setting. Default: 4
numratecats = 4

# (none, estimate, fixed) Specifies whether a parameter representing
# the proportion of sites that are unable to change (i.e. have a
# substitution rate of zero) will be included. This is typically
# referred to as “invariant sites”, but would better be termed
# “invariable sites”. Default: estimate
invariantsites = estimate

# ----------------------------------------------------------------------

[master]
nindivs = 4
holdover = 1
selectionintensity = .5
holdoverpenalty = 0

# The maximum number of generations to run.  Note that this supersedes
# the automated stopping criterion (see enforcetermconditions above),
# and should therefore be set to a very large value if automatic
# termination is desired.
stopgen = 1000000

# The maximum number of seconds for the run to continue.  Note that
# this supersedes the automated stopping criterion (see
# enforcetermconditions above), and should therefore be set to a very
# large value if automatic termination is desired.
stoptime = {stoptime}

startoptprec = 0.5
minoptprec = 0.01
numberofprecreductions = 20
treerejectionthreshold = 50.0
topoweight = 1.0
modweight = 0.05
brlenweight = 0.2
randnniweight = 0.1
randsprweight = 0.3
limsprweight =  0.6
intervallength = 100
intervalstostore = 5

limsprrange = 6
meanbrlenmuts = 5
gammashapebrlen = 1000
gammashapemodel = 1000
uniqueswapbias = 0.1
distanceswapbias = 1.0
"""

# ======================================================================

def generate_results(subtype, working_dir, best_tree_file):
    tjz = working_dir.joinpath(f"{subtype}.tjz")
    pdf = tjz.with_suffix(".pdf")
    subprocess.check_call(["tal", "--seqdb", "../seqdb.json.xz", "-D", "whocc", str(best_tree_file), str(tjz)], cwd=working_dir)
    subprocess.check_call(["tal", str(tjz), str(pdf)], cwd=working_dir)
    return [tjz, pdf]

# ======================================================================

def get_output_dir(working_dir, prefix):
    output_dirs = list(working_dir.glob(prefix + "*"))
    if output_dirs:
        output_dir = max(output_dirs)
    else:
        output_dir = working_dir.joinpath(prefix + datetime.datetime.now().strftime('%Y-%m%d-%H%M%S'))
        output_dir.mkdir()
        output_dir.chmod(0o777)
    return output_dir

# ----------------------------------------------------------------------

def log(working_dir, message):
    with working_dir.joinpath("log").open("a") as log:
        print(f"{datetime.datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: {message}", file=log)

# ======================================================================

import argparse, traceback

try:
    parser = argparse.ArgumentParser(description=__doc__)
    parser.add_argument('-d', '-v', '--debug', action='store_const', dest='loglevel', const=logging.DEBUG, default=logging.INFO, help='Enable debugging output.')

    parser.add_argument("subtype", choices=["h1", "h3", "bvic", "byam"])

    args = parser.parse_args()
    logging.basicConfig(level=args.loglevel, format="%(levelname)s %(asctime)s: %(message)s")
    exit_code = main(args)
except Error as err:
    logging.error(err)
    exit_code = 1
except Exception as err:
    logging.error(f"{err}\n{traceback.format_exc()}")
    exit_code = 1
exit(exit_code)

# ======================================================================
### Local Variables:
### eval: (if (fboundp 'eu-rename-buffer) (eu-rename-buffer))
### End:
